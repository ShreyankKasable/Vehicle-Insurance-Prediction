# ğŸš— MLOps Project - Vehicle Insurance Prediction Pipeline

Welcome to the **Vehicle Insurance MLOps Project** â€” a comprehensive end-to-end machine learning pipeline designed to automate, scale, and deploy predictive models for vehicle insurance classification. This project showcases practical implementation of **MLOps** principles including modular coding, cloud integration, CI/CD automation, and API deployment.

---

## ğŸ“ Project Setup and Structure

### âœ… Step 1: Generate Template

* Run `template.py` to scaffold the project structure with necessary folders and files.

### âœ… Step 2: Package Management

* Define local package structure using `setup.py` and `pyproject.toml`.
* ğŸ“„ See `crashcourse.txt` to understand these files.

### âœ… Step 3: Create and Activate Virtual Environment

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```

* Verify packages with:

```bash
pip list
```

---

## ğŸ“Š MongoDB Setup and Data Management

### ğŸ”§ Step 4: Setup MongoDB Atlas

1. Create a MongoDB Atlas account and M0 cluster.
2. Set up a DB user and whitelist IP as `0.0.0.0/0`.
3. Get the connection string for Python driver and update the password.

### ğŸ“„ Step 5: Push Dataset to MongoDB

* Create `notebook/mongoDB_demo.ipynb`, add your dataset and write code to upload it.
* Verify your uploaded data in Atlas > Browse Collections.

---

## ğŸ“ Logging, Exception Handling, and EDA

### ğŸ“Œ Step 6: Setup Logging and Exception Handling

* Create reusable logging and exception utilities.
* Test them in `demo.py`.

### ğŸ“ˆ Step 7: Perform EDA and Feature Engineering

* Analyze patterns and prepare features in your `EDA` notebook under `notebook/`.

---

## ğŸ“¥ Data Ingestion

### ğŸ”„ Step 8: Create Data Ingestion Pipeline

* Define:

  * MongoDB logic in `configuration.mongo_db_connections.py`
  * Raw data logic in `data_access/proj1_data.py`
  * Ingestion logic in `components.data_ingestion.py`
* Define config and artifact classes:

  * `entity/config_entity.py`
  * `entity/artifact_entity.py`

### ğŸŒ Step 9: Set MongoDB URL

```bash
# For Bash
export MONGODB_URL="mongodb+srv://<username>:<password>...."

# For PowerShell
$env:MONGODB_URL = "mongodb+srv://<username>:<password>...."
```

---

## ğŸ” Data Validation, Transformation & Model Training

### âœ… Step 10: Data Validation

* Add schema in `config/schema.yaml`
* Add validation logic in `utils/main_utils.py`

### âœ… Step 11: Data Transformation

* Build transformation code in `components/data_transformation.py`
* Add estimator logic in `entity/estimator.py`

### âœ… Step 12: Model Training

* Use transformed data to train models in `components/model_trainer.py`

---

## â˜ï¸ AWS Setup for Model Evaluation & Deployment

### ğŸ” Step 13: Configure AWS

1. Create IAM user with `AdministratorAccess`
2. Export AWS keys as environment variables:

```bash
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
```

### ğŸ©¢ Step 14: Model Registry using S3

* Create S3 bucket: `my-model-mlopsproj`
* Use:

  * `cloud_storage/aws_storage.py`
  * `entity/s3_estimator.py`
  * `constants/__init__.py` for bucket info

---

## ğŸš€ Model Evaluation, Pushing, and Prediction

### âœ… Step 15: Evaluate & Push Best Model

* Use `components/model_evaluation.py` and `model_pusher.py`

### ğŸŒ Step 16: Create FastAPI Prediction App

* Setup `app.py` with routes:

  * `/training`: Train pipeline
  * `/predict`: Make predictions

### ğŸ§± Step 17: Add Static Files

* Create `static/` and `template/` directories for frontend if needed

---

## ğŸ”„ CI/CD Automation with GitHub Actions, Docker, EC2

### ğŸ³ Step 18: Dockerize

* Create:

  * `Dockerfile`
  * `.dockerignore`

### ğŸ§· Step 19: GitHub Actions

* Setup `.github/workflows/aws.yaml`
* Add GitHub Secrets:

  * `AWS_ACCESS_KEY_ID`
  * `AWS_SECRET_ACCESS_KEY`
  * `ECR_REPO`
  * `AWS_DEFAULT_REGION`

### ğŸ—‚ï¸ Step 20: EC2 & Self-Hosted Runner

1. Launch EC2 (Ubuntu 24.04, T2 Medium)
2. Install Docker:

```bash
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
newgrp docker
```

3. Register EC2 as GitHub Runner

### ğŸŒ Step 21: Open EC2 Port

* Go to EC2 > Security > Edit Inbound Rules

  * Add: Custom TCP | Port 5080 | Source: 0.0.0.0/0

### âœ… Step 22: Access Application

* Visit: `http://<your-ec2-public-ip>:5080`

---

## ğŸ§± Folder Structure

```bash
.
â”œâ”€â”€ app.py
â”œâ”€â”€ demo.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ mongoDB_demo.ipynb
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schema.yaml
â”‚   â””â”€â”€ model.yaml
â””â”€â”€ src/
    â”œâ”€â”€ components/
    â”œâ”€â”€ configuration/
    â”œâ”€â”€ cloud_storage/
    â”œâ”€â”€ data_access/
    â”œâ”€â”€ constants/
    â”œâ”€â”€ entity/
    â”œâ”€â”€ exception/
    â”œâ”€â”€ logger/
    â”œâ”€â”€ pipeline/
    â””â”€â”€ utils/
```

---

## ğŸ› ï¸ Tech Stack

* **Python 3.10**
* **MongoDB Atlas**
* **FastAPI**
* **Scikit-learn**, **Pandas**, **NumPy**
* **Docker**, **GitHub Actions**, **EC2**, **S3**, **ECR**

---

## ğŸ¯ End-to-End Workflow

```text
Data Ingestion â Data Validation â Data Transformation â Model Training â
Model Evaluation â Model Registry (S3) â Deployment (EC2 + FastAPI + Docker) â CI/CD Pipeline
```

---

## ğŸ’¬ Author

**Shreyank Kasable**

ğŸŒ \[https://github.com/ShreyankKasable]

If you found this project helpful, donâ€™t forget to â­ the repo!
